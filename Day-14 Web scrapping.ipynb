{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51e4c31e-b307-4fbb-8c47-02275b3fb543",
   "metadata": {},
   "source": [
    "### Day 14 of programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e977d3ca-0e94-47a9-81a3-b5a5d0b77c13",
   "metadata": {},
   "source": [
    "## Python Tutorial: Web Scraping\n",
    "Introduction to Web Scraping\n",
    "Web scraping is the process of extracting data from websites. Python, with libraries such as requests and BeautifulSoup, makes it easy to fetch and parse web content. Web scraping can be used to collect data from websites for various applications such as data analysis, research, and automation.\n",
    "\n",
    "### Prerequisites\n",
    "Python installed on your system.\n",
    "\n",
    "Basic understanding of HTML and CSS.\n",
    "\n",
    "#### The following Python libraries:\n",
    "\n",
    "requests: To fetch web pages.\n",
    "\n",
    "beautifulsoup4: To parse HTML.\n",
    "\n",
    "You can install the necessary libraries using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8954cc-cfae-4d68-a586-971f07551488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.32.2)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (4.12.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2024.6.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1000)'))) - skipping\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f798cacd-56ed-446e-8770-bee6b39c460d",
   "metadata": {},
   "source": [
    "### Step 1: Fetching Web Pages with requests\n",
    "The requests library allows you to send HTTP requests and receive responses from the web. Here's a basic example of how to fetch a web page.\n",
    "\n",
    "Example: Fetching a Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47d2da62-4a7f-439e-b78e-4d31b8ee701d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page fetched successfully!\n",
      "<!doctype html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Example Domain</title>\n",
      "\n",
      "    <meta charset=\"utf-8\" />\n",
      "    <meta http-equiv=\"Content-type\" content=\"text/html; charset=utf-8\" />\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\" />\n",
      "    <style type=\"text/css\">\n",
      "    body {\n",
      "        background-color: #f0f0f2;\n",
      "        margin: 0;\n",
      "        padding: 0;\n",
      "        font-family: -apple-system, system-ui, BlinkMacSystemFont, \"Segoe UI\", \"Open Sans\", \"Helvetica Neue\", Helvetica, Arial, sans-serif;\n",
      "        \n",
      "    }\n",
      "    div {\n",
      "        width: 600px;\n",
      "        margin: 5em auto;\n",
      "        padding: 2em;\n",
      "        background-color: #fdfdff;\n",
      "        border-radius: 0.5em;\n",
      "        box-shadow: 2px 3px 7px 2px rgba(0,0,0,0.02);\n",
      "    }\n",
      "    a:link, a:visited {\n",
      "        color: #38488f;\n",
      "        text-decoration: none;\n",
      "    }\n",
      "    @media (max-width: 700px) {\n",
      "        div {\n",
      "            margin: 0 auto;\n",
      "            width: auto;\n",
      "        }\n",
      "    }\n",
      "    </style>    \n",
      "</head>\n",
      "\n",
      "<body>\n",
      "<div>\n",
      "    <h1>Example Domain</h1>\n",
      "    <p>This domain is for use in illustrative examples in documents. You may use this\n",
      "    domain in literature without prior coordination or asking for permission.</p>\n",
      "    <p><a href=\"https://www.iana.org/domains/example\">More information...</a></p>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.example.com'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    print(\"Page fetched successfully!\")\n",
    "    print(response.text)  # Print the HTML content of the page\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55fb2e99-67ab-4c05-9a59-606a976f2339",
   "metadata": {},
   "source": [
    "### Step 2: Parsing HTML with BeautifulSoup\n",
    "Once you have fetched the HTML content, you can use BeautifulSoup to parse and extract data from it.\n",
    "\n",
    "Example: Parsing HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57549dd-c438-444d-9585-97e51c4a04e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Test Page\n",
      "Paragraph text: This is a paragraph.\n",
      "Paragraph text: Another paragraph with a class.\n",
      "Info paragraph text: Another paragraph with a class.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Sample HTML content (for demonstration purposes)\n",
    "html_content = \"\"\"\n",
    "<html>\n",
    "<head><title>Test Page</title></head>\n",
    "<body>\n",
    "<h1>Welcome to the Test Page</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "<p class=\"info\">Another paragraph with a class.</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Create a BeautifulSoup object and specify the parser\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Extract the title of the page\n",
    "title = soup.title.string\n",
    "print(f\"Title of the page: {title}\")\n",
    "\n",
    "# Extract all paragraph tags\n",
    "paragraphs = soup.find_all('p')\n",
    "for p in paragraphs:\n",
    "    print(f\"Paragraph text: {p.get_text()}\")\n",
    "\n",
    "# Extract text of a paragraph with a specific class\n",
    "info_paragraph = soup.find('p', class_='info')\n",
    "print(f\"Info paragraph text: {info_paragraph.get_text()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743da0c9-69d0-40aa-acae-bf268acc23a2",
   "metadata": {},
   "source": [
    "### Step 3: Combining requests and BeautifulSoup\n",
    "To scrape data from a real web page, combine requests to fetch the page and BeautifulSoup to parse the HTML.\n",
    "\n",
    "Example: Scraping a Real Web Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd2359c-d4cd-41c9-b9e9-3c2bbbffc8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title of the page: Example Domain\n",
      "Heading text: Example Domain\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the page you want to scrape\n",
    "url = 'https://www.example.com'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Extract and print the title of the page\n",
    "    title = soup.title.string\n",
    "    print(f\"Title of the page: {title}\")\n",
    "    \n",
    "    # Extract and print all headings (h1, h2, h3)\n",
    "    headings = soup.find_all(['h1', 'h2', 'h3'])\n",
    "    for heading in headings:\n",
    "        print(f\"Heading text: {heading.get_text()}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a048b20-b64c-4531-82e5-1f60262bd9e8",
   "metadata": {},
   "source": [
    "#### Explanation of the Code:\n",
    "response.text:\n",
    "\n",
    "This is the HTML content of the web page that you fetched using requests.get(). The requests.get(url) call sends a request to the server to retrieve the content of the page, and response.text contains the raw HTML as a string.\n",
    "BeautifulSoup():\n",
    "\n",
    "This is a constructor from the BeautifulSoup library. It is used to create a BeautifulSoup object that parses the raw HTML content.\n",
    "BeautifulSoup() takes two main arguments:\n",
    "HTML content (in this case, response.text).\n",
    "\n",
    "Parser type ('html.parser' in this case), which specifies how to interpret and parse the HTML code.\n",
    "'html.parser':\n",
    "\n",
    "This tells BeautifulSoup which parser to use for parsing the HTML.\n",
    "'html.parser' is the default parser included with Python. It is used to convert the HTML document into a nested data structure (like a tree), which makes it easier to search and navigate through the HTML elements (tags, attributes, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d27f858-279d-40e7-b568-5e004496d7fc",
   "metadata": {},
   "source": [
    "### Step 4: Handling Common Issues\n",
    "Respecting robots.txt: Always check the robots.txt file of a website to ensure you're allowed to scrape it. This file specifies rules for web crawlers.\n",
    "\n",
    "Handling HTTP Errors: Always check the status code of the response. Common codes include:\n",
    "\n",
    "200: OK\n",
    "\n",
    "404: Not Found\n",
    "\n",
    "403: Forbidden\n",
    "\n",
    "Dealing with Dynamic Content: Some websites use JavaScript to load content dynamically. For these, consider using tools like Selenium to interact with the web page.\n",
    "\n",
    "Rate Limiting: Avoid sending too many requests in a short time to prevent being blocked. Implement delays between requests if scraping multiple pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f505bc-b055-47b0-b238-97269e68fbaf",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Fetching Web Pages: Use requests to send HTTP requests and retrieve web pages.\n",
    "\n",
    "Parsing HTML: Use BeautifulSoup to parse and extract data from HTML content.\n",
    "\n",
    "Handling Common Issues: Respect robots.txt, handle HTTP errors, and manage dynamic content appropriately.\n",
    "\n",
    "Practice: Implement and test your scraping skills through various practical exercises.\n",
    "\n",
    "Web scraping is a valuable tool for data collection and analysis. With practice and careful handling of web content, you can effectively gather and use data from the web."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ea800-f1dd-4d76-a334-2e097b7bc01f",
   "metadata": {},
   "source": [
    "## Practice Questions\n",
    "\n",
    "1. Write a Python script that logs into a website and scrapes data from a page that requires authentication. Use the requests library for handling login.\n",
    "2. Write a Python script to scrape data from a website and perform basic data analysis (e.g., average price of products, most common categories).\n",
    "3. Write a Python script to scrape data from a website (e.g., product information) and save the extracted data to a CSV file.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
